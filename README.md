# HPC-basic-knowledge
Hello world, hello Ranran.

## 推理加速框架-以GPU加速为例<br>
### 一、是什么<br>
AI 推理是将用户输入的数据，通过训练好的模型产生有价值信息的过程。具体是指将训练好的 AI 模型部署到提供算力的硬件上，并通过 HTTP/RPC 等接口对外提供服务。<br>
### 二、为什么<br>
用户对 GPU 的使用初始于业务系统，用户根据业务需求搭建模型，并为最终模型的效果负责。<br>

业务系统构建完成后，会从资源管理系统中申请资源，而资源管理器则会将 GPU 卡分配给业务系统，这个管理器只会为资源分配率负责，而不会关心资源分配后的业务使用效率。用户在申请到资源后，会通过 AI 框架执行模型的计算过程。

AI 框架团队更专注为用户提供易用的模型构建接口，而不会为业务的推理效率和资源利用率负责。AI 框架团队在使用异构硬件算力时，只能使用基础的加速包或工具，而不会专门结合业务特点进行优化。总的来看，整个过程中没有专门的工具为 GPU 算力的利用效率负责。<br>

为此，我们需要 AI 推理加速，针对用户训练好的模型，进行针对性的加速，缩短业务推理时间，同时提升资源利用率。<br>
### 三、怎么办<br>
#### 如果想要提升GPU的算力，核心是：<br>
1、保持GPU上有任务：GPU 利用率被定义为在采样间隔内，GPU 上有任务在执行的时间。<br>
2、同时对单个 GPU 计算任务，使其可以尽量充分的用好 SM 处理器：SM 利用率则被定义为在采样间隔内，每个 SM 有 warp 活跃时间的平均值。<br>
SM 利用率可以更精准地反应计算任务效率不高的问题，因为一个GPU可以有好几个SM处理器。<br>
#### 优化方案：<br>
##### * 模型执行之前<br>
模型精简：即在模型真正执行之前就对模型量进行精简，来提升推理速度。常见的优化方向包括量化、减枝、蒸馏和 NAS 等<br>
###### a.量化：量化是指将模型中的计算类型进行压缩，进而减小计算量（int8要比float少一半字节）<br>
分为：<br>
（1）离线量化<br>
（2）量化训练<br>

##### * 模型已经交由推理引擎在GPU上跑了，提高GPU的利用率<br>
占用率公式：每个SM中活跃线程束的数量/每个SM中最大的线程束的数量<br>
1. 尽可能让GPU上有计算任务：算子融合<br>
2. 单个计算任务在GPU上执行效率更高：单算子优化<br>
